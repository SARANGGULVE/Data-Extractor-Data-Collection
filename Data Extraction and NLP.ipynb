{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t6TX1SBSurz9","executionInfo":{"status":"ok","timestamp":1727444782069,"user_tz":-330,"elapsed":21967,"user":{"displayName":"Sarang Gulve","userId":"12559955847416367101"}},"outputId":"d21880af-a7df-4a0c-dc51-f1fd6b234ce4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mjiOFyqum8Yi","outputId":"5cd3995c-d7e6-4c4f-9c01-99b02eca900f","executionInfo":{"status":"ok","timestamp":1727445197750,"user_tz":-330,"elapsed":22032,"user":{"displayName":"Sarang Gulve","userId":"12559955847416367101"}}},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","<ipython-input-2-640dc6179709>:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  output_df = pd.concat([output_df, new_row], ignore_index=True)\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Failed to retrieve https://insights.blackcoffer.com/monday-com-to-kpi-dashboard-to-manage-view-and-generate-insights-from-the-crm-data/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/monday-com-to-kpi-dashboard-to-manage-view-and-generate-insights-from-the-crm-data/\n"]}],"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import re\n","import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","\n","def load_master_dictionary():\n","    with open('/content/drive/MyDrive/positive-words.txt', 'r', encoding='latin-1') as file:\n","        positive_words = set(file.read().splitlines())\n","\n","    with open('/content/drive/MyDrive/negative-words.txt', 'r', encoding='latin-1') as file:\n","         negative_words = set(file.read().splitlines())\n","\n","    return positive_words, negative_words\n","\n","\n","def load_custom_stopwords():\n","    stopwords_set = set()\n","\n","\n","    with open('/content/drive/MyDrive/StopWords/StopWords_Auditor.txt', 'r', encoding='latin-1') as file:\n","        stopwords_set.update(file.read().splitlines())\n","\n","    with open('/content/drive/MyDrive/StopWords/StopWords_Currencies.txt', 'r', encoding='latin-1') as file:\n","        stopwords_set.update(file.read().splitlines())\n","\n","    with open('/content/drive/MyDrive/StopWords/StopWords_DatesandNumbers.txt', 'r', encoding='latin-1') as file:\n","        stopwords_set.update(file.read().splitlines())\n","\n","    with open('/content/drive/MyDrive/StopWords/StopWords_Generic.txt', 'r', encoding='latin-1') as file:\n","        stopwords_set.update(file.read().splitlines())\n","\n","    with open('/content/drive/MyDrive/StopWords/StopWords_GenericLong.txt', 'r', encoding='latin-1') as file:\n","        stopwords_set.update(file.read().splitlines())\n","\n","    with open('/content/drive/MyDrive/StopWords/StopWords_Geographic.txt', 'r', encoding='latin-1') as file:\n","        stopwords_set.update(file.read().splitlines())\n","\n","    with open('/content/drive/MyDrive/StopWords/StopWords_Names.txt', 'r', encoding='latin-1') as file:\n","        stopwords_set.update(file.read().splitlines())\n","\n","    return stopwords_set\n","\n","def clean_text(text):\n","\n","    nltk_stopwords = set(stopwords.words('english'))\n","    custom_stopwords = load_custom_stopwords()\n","\n","    all_stopwords = nltk_stopwords.union(custom_stopwords)\n","\n","    words = word_tokenize(text)\n","\n","    cleaned_words = [word for word in words if word.isalnum() and word.lower() not in all_stopwords]\n","\n","    return cleaned_words\n","\n","def sentiment_analysis(cleaned_words, positive_words, negative_words):\n","    positive_score = sum(1 for word in cleaned_words if word.lower() in positive_words)\n","    negative_score = sum(1 for word in cleaned_words if word.lower() in negative_words)\n","\n","    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n","\n","    subjectivity_score = (positive_score + negative_score) / (len(cleaned_words) + 0.000001)\n","\n","    return positive_score, negative_score, polarity_score, subjectivity_score\n","\n","def readability_analysis(text):\n","    sentences = sent_tokenize(text)\n","    total_words = word_tokenize(text)\n","\n","    avg_sentence_length = len(total_words) / len(sentences) if len(sentences) > 0 else 0\n","\n","    complex_words = [word for word in total_words if count_syllables(word) > 2]\n","    percentage_complex_words = len(complex_words) / len(total_words) if len(total_words) > 0 else 0\n","\n","    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n","\n","    return avg_sentence_length, percentage_complex_words, fog_index\n","\n","def count_syllables(word):\n","    word = word.lower()\n","    vowels = \"aeiou\"\n","    count = sum(1 for char in word if char in vowels)\n","    if word.endswith(('es', 'ed')):\n","        count -= 1\n","    return max(count, 1)\n","\n","def word_analysis(text):\n","    sentences = sent_tokenize(text)\n","    words = word_tokenize(text)\n","\n","    word_count = len(words)\n","    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n","\n","    total_characters = sum(len(word) for word in words)\n","    avg_word_length = total_characters / len(words) if len(words) > 0 else 0\n","\n","    return len(sentences), word_count, personal_pronouns, avg_word_length\n","\n","def extract_article_text(url):\n","    try:\n","\n","        response = requests.get(url)\n","        response.raise_for_status()\n","\n","        soup = BeautifulSoup(response.content, 'html.parser')\n","\n","        title = soup.find('h1').get_text(strip=True) if soup.find('h1') else \"\"\n","\n","        article_body = soup.find('article').get_text(separator=' ', strip=True) if soup.find('article') else \"\"\n","\n","        full_text = title + \"\\n\\n\" + article_body\n","\n","        return full_text\n","\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Failed to retrieve {url}: {e}\")\n","        return \"\"\n","\n","input_df = pd.read_excel('/content/drive/MyDrive/Input.xlsx')\n","\n","positive_words, negative_words = load_master_dictionary()\n","\n","output_columns = ['URL_ID', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH',\n","                  'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT',\n","                  'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH', 'URL']\n","output_df = pd.DataFrame(columns=output_columns)\n","\n","for index, row in input_df.iterrows():\n","    url_id = row['URL_ID']\n","    article_url = row['URL']\n","\n","    article_text = extract_article_text(article_url)\n","\n","    cleaned_words = clean_text(article_text)\n","\n","    positive_score, negative_score, polarity_score, subjectivity_score = sentiment_analysis(cleaned_words, positive_words, negative_words)\n","\n","    avg_sentence_length, percentage_complex_words, fog_index = readability_analysis(article_text)\n","\n","    num_sentences, word_count, personal_pronouns, avg_word_length = word_analysis(article_text)\n","\n","    complex_word_count = sum(1 for word in cleaned_words if count_syllables(word) > 2)\n","\n","    syllables_per_word = sum(count_syllables(word) for word in cleaned_words) / len(cleaned_words) if len(cleaned_words) > 0 else 0\n","\n","    new_row = pd.DataFrame({\n","        'URL_ID': [url_id],\n","        'POSITIVE SCORE': [positive_score],\n","        'NEGATIVE SCORE': [negative_score],\n","        'POLARITY SCORE': [polarity_score],\n","        'SUBJECTIVITY SCORE': [subjectivity_score],\n","        'AVG SENTENCE LENGTH': [avg_sentence_length],\n","        'PERCENTAGE OF COMPLEX WORDS': [percentage_complex_words],\n","        'FOG INDEX': [fog_index],\n","        'AVG NUMBER OF WORDS PER SENTENCE': [avg_sentence_length],\n","        'COMPLEX WORD COUNT': [complex_word_count],\n","        'WORD COUNT': [word_count],\n","        'SYLLABLE PER WORD': [syllables_per_word],\n","        'PERSONAL PRONOUNS': [personal_pronouns],\n","        'AVG WORD LENGTH': [avg_word_length],\n","        'URL': [article_url]\n","    })\n","    output_df = pd.concat([output_df, new_row], ignore_index=True)\n","\n","output_df.to_excel('/content/drive/MyDrive/Output_Structure.xlsx', index=False)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"vQy1efGXvN-C"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1qgwEkg2K-hXWwLGDLmApBt2bxq-ivmyE","authorship_tag":"ABX9TyPW41OYj9+TrHuY5Y2XwN1K"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}