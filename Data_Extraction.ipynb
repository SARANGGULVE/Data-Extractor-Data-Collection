{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1128,"status":"ok","timestamp":1727447137895,"user":{"displayName":"Sarang Gulve","userId":"12559955847416367101"},"user_tz":-330},"id":"fjssa1WY3SBO"},"outputs":[],"source":["import pandas as pd\n","\n","input_file = \"/content/drive/MyDrive/Input.xlsx\"\n","data = pd.read_excel(input_file)\n","\n","urls = data['URL']\n","url_ids = data['URL_ID']"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":960,"status":"ok","timestamp":1727447184298,"user":{"displayName":"Sarang Gulve","userId":"12559955847416367101"},"user_tz":-330},"id":"t3ZB8Itx3hhE"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","\n","def extract_article(url):\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","\n","    title = soup.find('h1').get_text() if soup.find('h1') else ''\n","    article_text = soup.find('article').get_text(separator=' ') if soup.find('article') else ''\n","\n","    return title, article_text\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TGD9N8RR4CzJ"},"outputs":[],"source":["for i, url in enumerate(urls):\n","    url_id = url_ids[i]\n","    title, text = extract_article(url)\n","\n","    with open(f\"{url_id}.txt\", \"w\", encoding=\"utf-8\") as f:\n","        f.write(title + \"\\n\\n\" + text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z1OlS5hN4YJ3"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOs75S2tmT05ZSOchrcrh83","mount_file_id":"1RqlDsyT_n8CNGWjvp43rIh-go28OotJo","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}